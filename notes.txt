Monolithic app.
- running as a single process spread across a handful of servers.
- slow release cycles.
- "package up the whole system" and deploy.
- components "tightly coupled together" and have to be developed, deployed, and manged "as one entity".
- can't scale components individually.

Vertical Scaling
- by adding more CPUs, memory to a server.

Horizontal Scaling
- by adding additional servers (replicas)

Microservices. (ms)
- monolithic app being broken down into components (microservices)
- decoupled from each other.
- developed, deployed, updated and scaled individually.
- bigger numbers of components hard to configure, manage, and keep.
- talk through HTTP (sync) or AMQP (async) or IPC

Divergence of environment (배포 환경 차이의 증가)
- 마이크로서비스는 독립적이기 때문에 필요한 의존 라이브러리를 자유롭게 결정.
- 문제는 "하나의 노드"에 여러 마이크로서비스를 배포시 중복 라이브러리로 의해 작업 복잡성 증가.
- The bigger the number of components need to deploy on the same node, the harder it will be to manage all deps.

DevOps
- a team develops and deploying and taking care of the app.

Container Technologies. (Docker, rkt)
- to provide "isolation apps" (env, deps, network, filesystem) instead of vm.
- achieve by using linux namespace & cgroup

    Linux namespaces.
    - makes sure each process sees its own "personal view" of the system.
    - mnt
    - pid   (process id)
    - net   (network)
    - ipc   (inter-process communication)
    - user
        
Kube (Kubernetes)
- a platform that allows you to deploy and manage containerized apps.
- run apps on one or more nodes.
- provides service discovering, scaling, load-balancing, self-healing and leader election (master-slave model). 
- abstract away the underlying infrastructure and expose whole data-center as a single computational resource (cluster).
- composed of a master node and one or more worker nodes.

Kube architecture.
    Hardware level.
        Master Node
        - hosts the "Kube Control Plane" which controls and manage the whole kube system.
        - composed of etcd, Kube Api Server, Scheduler, Controller Manager
            
        Worker Node
        - run the actual app you deploy (by "kubelet")
        - composed of "kubelet", "kube-proxy", "container runtime (docker)"
        
Kube Master Components
    Kube Control Plane
    - controls the kube cluster and makes it function.
    - composed of etcd, Kube Api Server, Scheduler, Controller Manager
    
    Kube Api Server (KAS)
    - an entry point allow client and other components in the Control Plan communicate with.
    
    Scheduler
    - schedules apps to supervision ("assign a worker node" to deployable components of your app)
    - "scheduling" means assigning the pod to a node.
    
    Controller Manager.
    - performs cluster-level functions. (keeps replica, track of worker nodes, handling node failures)

    etcd
    - a distributed key-value data store that stores the cluster configuration (resource manifests).

Kube Worker Node Components.
    Container Runtime
    - runs you containers.
    
    Kubelet
    - talk with API server and manages containers on that node.
    
    kube-proxy
    - load-balances network traffic btw app components.

Docker 
- a platform for packaging, distributing, and running containerized apps.
- make containers "portable" across different nodes.

    Docker components.
    - Images
        - something you package the app and its env into.
        - container images are composed of layers that can be shared among different images.
        - docker run
        - docker build
        - docker ps, images
        
    - Registries
        - place to store your images and share with other developer.
        
    - Containers
        - regular linux container (process) created from your image.
        - isolated from the node and all other processes.
        - using its own PID Linux namespace. (completely isolated process  tree)
        - has an isolated filesystem.
        
kubectl
- interact with the kube through the "kubectl cli client".
- issues REST requests to the Kube API server on the master node.

pods
- co-located group of containers on the same worker node and in the same Linux namespace.
- a logical machine which owns IP, hostname, processes.
- containers in a pod run on a single worker node.
- Pods are spread out across different worker nodes. (flat network)
- basic building block in kube.
- pods are ephemeral. (can disappear at any time)
- creating by posting a JSON manifest to "Kube API server"
- container logs are automatically rotated daily and every time the log file reaches 10MB in size.

apiVersion: v1
kind: Pod
metadata:
  name: [podName]
  labels:
    [pod's label]   # attached labels for organizing purpose
spec:
  containers:
    - name: [ctName]    # image to create the container from
      image: [img]
      ports:
        - containerPort: [ctPort]   # the port the container is listening on
          protocol: TCP
  
    Sequence of deploying a pod 
    1. developer build & push the img
    2. developer issues pod's manifest to KAS through "kubectl"
    3. Scheduler notified the event and schedule the pod to a node.
    4. kubelet is notified and run the img through "Container Runtime" on the node.

    CONTAINERS IN A POD 
    - partially isolated.
    - share the same hostname and network interfaces (IP address and port).
    - the filesystem of each is fully isolated from others.
    
    FLAT INTER-POD Network. (No NAT)
    - ALL pods in a single flat, shared, network-address space.
    - each pod gets a routable IP and all other pods see the pod under that IP address.
    
    REASON TO SPLIT MULTIPLE CONTAINERS INTO MULTIPLE PODS
    - taking advantage of cluster's computational resource.
      컨테이너가 두개일땐, 두 팟으로 나눠 각 노드에 하나씩 할당하 리소스 활용성을 높일 수 있지만 하나라면 불가.
    - horizontally scale individual pods.
      컨테이너가 두개일땐, 필요한 컨테이너만 스케일을 따 증가시킬 수 있지만 하나라면 불가.
   
   WHEN TO USE MULTIPLE CONTAINERS IN A POD
   - pods should contain tightly coupled containers. (main container and supports - etc log collectors.)
 
    STRUCTURE OF YAML DESCRIPTOR
    - metadata, spec, status.
    
    apiVersion:     # Kube API version
    kind:           # type of resource
    metadata:       # Resource metadata [name, namespace, labels, annotations]
        ...
    spec:           # Resource specification (containers, volumes, security, networking..)
        ...
    status:         # Detailed current status of the resource

    PORT FORWARDING (kube port-forward)
    - forwarding a local port to a port in the port. 
    - an effective way to test an pod.
    
    ORGANIZING RESOURCES
    - LABELS, NAMESPACE
         LABELS
        - grouping resources into subsets based on criteria. (by function, env, version, feature..)
        - key-value pair attached to a resource.
        - label is utilized by "label selectors"
        
            LABEL SELECTOR
            - allow you to select a subset of pods tagged with certain labels.
            
            NOT KEY
            - !key
            NOT
            - key!=value
            IN  
            - key in (value1, value2...)
            NOTIN
            - key notin (value1, value2...) 
        
        NAMESPACE
        - groups objects into namespaces.
        - separate resources that don't belong together into non-overlapping groups.
        - still resources in namespace can communicate with others.
        
    Liveness Probe
    - periodically check whether a container is still alive and restart if the probe fails.
    - specify the probes for each container in a pod.
    - configure the probe by delay, timeout, period.
    - Liveness probe cannot solve the node failure scenario
    
        HTTP GET Probe
        - performs an HTTP GET request on the container's IP address, a port and path you specify.
        
        TCP Socket Probe
        - try to open a TCP connection to the specified port.
        
        Exec Probe 
        - execute an arbitrary command inside the container.
        
    apiVersion: v1
    kind: Pod
    metadata:
        ...
    spec:
      containers:
        - name: main
          image: sm123tt/backend:v1
          ports:
            ...
          livenessProbe:
            initialDelaySeconds: 15 # wait 15 seconds before executing the first probe
            timeoutSeconds: 3
            periodSeconds: 10
            failureThreshold: 5
            httpGet:
              port: http
              path: /fail
    
    Ready:          True
    Restart Count:  4
    Liveness:       http-get http://:http/fail delay=0s timeout=1s period=10s #success=1 #failure=3

RC (ReplicationController)
- replicate pods and keep the number of replica.
- ensures the pods managed by rc are always kept running. (node fail or the pod was evicted)
- pods managed by rc is determined by label & label selector.
- use RS (ReplicaSet) instead of RC (rs provides better selector)
- other similar resources are CronJob (scheduled task), Job (completable task), DaemonSet (pod only on certain node)

apiVersion: apps/v1
kind: ReplicaSet        # defines RS
metadata:
    ...
spec:
  replicas: 3           # the desire number of pod instances
  selector:             # the selector to determining managed pod by this rc
    matchExpressions:
      - key: app
        operator: In
        values: ["kubia"]
  template:             # pod's template
    metadata:
      name: kubia
      labels:
        app: kubia      # should match with the above selector's criteria
    spec:
      containers:
        - name: kubia
          ...
          livenessProbe:
            initialDelaySeconds: 10
            httpGet:
              port: http
              path: /
              
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       4m30s

    RC Components.
    - label selector - determines what pods are in the rc's scope
    - replica        - the desired number of pods
    - pod template   - is used when creating new pod replicas.
     
     Effect of Changing The Current rc's Label Selector or Pod Template
     - no effect on existing pods
     - label selector makes the existing pods fall out of the scope. (will create new replicas)
     - The template only affects new pods but not old pods. (cookie cutter)
     
    Benefit of RC
    - failure over
    - horizontal scaling
    
    Moving Pods Out Of the Scope of a RC
    - changing a pod's label, it can be removed from rc's scope.
    - good technique for debugging the buggy pods.
    
    Scaling a RC
    - kube scale or kube edit
    
Service.
- constant entrypoint which gets a static IP for multiple pods or rc.
- redirect & load balancing client requests to the pods.
- allow to find other pod by its name through env variables or DNS.
- ClusterIP, NodePort, LoadBalancer, Ingress types are available.

apiVersion: v1
kind: Service
metadata:
  name: kubia
  labels:
    app: kubia
spec:
  sessionAffinity: ClientIP # change behavior of the service proxy 
  selector:             # All pods with the label will be backed by the service
    app: kubia
  ports:
    - port: 80          # the service will be available on the port
      name: http        # named port. (mapped to the container's port called "http")
      targetPort: http  # the container port the service forward to
    - port
      name: https
      targetPort: https # multi ports can be configured.

    Service Components
    - Cluster IP: is only accessible from inside the cluster.
    - sessionAffinity: makes the service proxy redirect all requests originating from the same client IP to the same pod.
    - targetPort: he container port the service forward to
    - port: the service will be available on the port
    - nodePort: node opens a port and redirects traffic to the underlying service.
    - Discovering Services: allow client's pod know the IP and port of a service through env or "Kube DNS"
    - Endpoints.
    
    Discovering Service
    ENV
        ...
        KUBERNETES_SERVICE_PORT_HTTPS=443
        KUBIA_SERVICE_HOST=10.8.0.136   # clusterIP of the service
        KUBIA_SERVICE_PORT_HTTPS=443    # the pod the service is available on
        KUBIA_PORT_80_TCP_PORT=80
    DNS 
    - "kube-dns" pod in the namespace "kube-system" provides dns service on the cluster infra.
    - all the pods are configured to use the dns server (/etc/resolv.conf)
    
        [svc].[namespace].svc.cluster.local [FQDN]
        [svc].[namespace]
        [svc]
        
    Endpoints.
    - Endpoint represent ip address and port of a pod.
    
    Name:              kubia
    Namespace:         default
    Labels:            app=kubia
    Annotations:       cloud.google.com/neg: {"ingress":true}
    Selector:          app=kubia
    Type:              ClusterIP
    IP:                10.8.0.136
    Port:              http  80/TCP
    TargetPort:        http/TCP
    Endpoints:         10.4.0.193:8080,10.4.0.194:8080,10.4.2.93:8080   # the list of pod IPs and ports that represent the endpoints of this service.
    

    Expose services living outside the cluster. 
    - 클러스터 외부에 존재하는 서비스를 클러스 내부에 expose.
    - client pods in the cluster can connect to the external service like they connect to internal services.
   
    apiVersion: v1
    kind: Service
    metadata:
      name: google
      labels:
        app: google
    spec:               # don't specify selector.
      ports:
        - port: 80
    ---
    apiVersion: v1
    kind: Endpoints
    metadata:
      name: google      # the name of the ep must match the name of the service.
      labels:
        app: google
    subsets:
      - addresses:
          - ip: 172.217.174.206 # IPs of the ep
        ports:
          - port: 80            # target port of the endpoints
         
  Exposing Services to External Clients
      NodePort
      - opens a port on the node and redirects traffic to the service.
      - reserves a port on all its node for the service.
      - can access through [svcClusterIP]:[svcPort] or [nodeExternalIP]:[nodePort]
      
      apiVersion: v1
      kind: Service
      metadata:
        name: extweb
        labels:
          app: extweb
          rel: beta
      spec:
        type: NodePort      # set the service type NodePort
        selector:
          app: extweb
        ports:
          - port: 80
            nodePort: 30123 # the service will be accessible throught port 30123 of each of the nodes.
            name: http
            targetPort: http
      
      LoadBalancer
      - makes service accessible through a load balancer (provided from cloud sp)
      - LB has its own unique, public IP address and will redirect 
      
      apiVersion: v1
      kind: Service
      metadata:
        name: extweb
        labels:
          app: extweb
          rel: beta
      spec:
        type: LoadBalancer
        selector:
          app: extweb
        ports:
          - port: 80
            targetPort: 8080
            
      Ingress
      - exposing "multiple services" through a single IP address.
      - Multiple services can be exposed through a single ingress (with a single public IP)
      - operate at the application layer (OSI model) and can provide cookie-based session affinity.
      
        Ingress vs LB
        - LB service requires its own LB with its own public IP address
        - Ingress only requires one, even when providing access to dozen of services.
        
        apiVersion: extensions/v1beta1
        kind: Ingress
        metadata:
          name: ingress
          labels:
            app: ingress
            chapter: 5-13
        spec:
          rules:
            - host: kubia.example.com   # ingress map the host name to the service
              http:
                paths:
                - backend:
                    serviceName: user-api
                    servicePort: 80
                  path: /user-api
                - backend:
                    serviceName: kubia
                    servicePort: 80
                  path: /kubia
        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: user-api
          labels:
            app: user-api
            chapter: 5-13
        spec:
          type: NodePort
          ports:
            - port: 80
              nodePort: 30124
              name: http
              targetPort: http
      
      Ingress TLS certificate
      - the communication between client and the server is encrypted.
      - need to attach server's certificate and a private key to the Ingress. (through secret)
      
      kube create secret tls [secretName] --cert=[cert] --key=[key]
      
      apiVersion: extensions/v1beta1
      kind: Ingress
      metadata:
        name: ingress
        labels:
          app: ingress
          chapter: 5-13
      spec:
        tls:
          - hosts: ["kubia.example.com"]    # TLS connections will be accepted for this hostname
            secretName: tls-secret          # secret for the tls key and cert
        rules:
     
     Ingress & Readiness Probes.
     - when the pod is created, it might not be ready to serve the service.
     - readiness probe is invoked periodically and determines whether the pod is ready to serve or not.
     - the pod won't be killed but it will not get any request until it's up again.
     - three types of probe. exec, http get, tcp
     
     Troubleshooting services.
     - make sure connection for service's cluster IP
     - define a readiness probe, make sure it's succeeding.
     - make sure that a pod is part of the service. (kube get ep)
     - make sure the discovering service work through its FQDN
     
Volumes
- must be mounted in each container to use it.
- data storage that can be shared with other containers.
- variety of life cycle.

    VolumeTypes
        emptyDir 
        - a simple empty directory used for storing transient data
        - the volume's lifetime is tied to the pod.
        
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
            ...
        spec:
            ...
            spec:
              containers:
                - name: html-generator
                  image: sm123tt/fortune:v1
                  imagePullPolicy: Always
                  volumeMounts:
                    - mountPath: /var/htdocs # the html volume is mounted at /var/htdocs
                      name: html
                  readinessProbe:
                    initialDelaySeconds: 10
                    exec:
                      command:
                        - "ls"
                - name: web-server
                  image: nginx:alpine 
                  volumeMounts:     
                    - mountPath: /usr/share/nginx/html  # the same volume as above is mounted as read-only.
                      name: html
                      readOnly: true
                  ports:
                    - containerPort: 80
                      name: http
                      protocol: TCP
                  readinessProbe:
                    initialDelaySeconds: 10
                    httpGet:
                      port: http
                      path: /
              volumes:
                - name: html        // a single emptyDir volume called html
                  emptyDir: {}
                  
        hostPath
        - volume points to a specific file or directory on the node's filesystem
        - pods running on the same node will see the same files.
        - the volume's lifecycle is tied to the node.
        
        cloud disks (gcePersistentDisk, awsElasticBlockStore, azureDisk): used for mounting cloud specific storage.
            gcePersistentDisk
            - use GCE Persistent Disk as underlying storage mechanism.
            - the volume's lifecycle is independent.
            
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
            ...
        spec:
            ...
          template:
            metadata:
              name: mongodb
              labels:
                app: mongodb
                chapter: "6"
            spec:
              containers:
                - name: db
                  image: mongo
                  volumeMounts:
                    - mountPath: /data/db   // mongodb-data volume is mounted at /data/db
                      name: mongodb-data
              volumes:
                - name: mongodb-data
                  gcePersistentDisk:        // type of volume is GCE Persistent disk.
                    pdName: mongodb         // the name of the the persistent disk
                    fsType: ext4            // the filesystem is EXT4.
                    
        - configMap, secret, downwardAPI: used for cluster or kube resources info.
    
        PersistentVolume (pv) & PersistentVolumeClaim (pvc) & storageClass
        - decoupling pods from the underlying network storage infrastructure.
        
        PersistentVolume (pv)
        - represents data storage which is provisioned by cluster admins and consumed by pods through pvc
        - deleting bounded pvc doesn't affect pv to be deleted. (Released Status)
            다른 pvc 가 바인딩하여 덮어쓰는 방식의 데이터의 유실을 막기 위해.
            
            PersistentVolume components.
            - disk size
            - underlying storage spec
            - accessModes.
             
        PersistentVolumeClaim (pvc)
        - a claim for data storage from pod and Kube finds the appropriate pv and binds the volume to the claim.
        - pvc never know about it's pv
        
            PersistentVolumeClaim components.
            - request disk size
            - accessMode to use
            - storageClassName
        
        PVC's Access Modes
        RWO - read & writing for a single client.
        ROX - read only for multiple clients.
        RWX - read & writing for multiple clients.
        
        pv & pvc
        apiVersion: v1
        kind: PersistentVolume
        metadata:
          name: mongodb
          labels:
            app: mongodb
            chapter: "6"
        spec:
          capacity:
            storage: 1Gi    # pv's disk size
          accessModes:      
            - ReadWriteOnce # a single client for writing and reading
            - ReadOnlyMany  # multiple clients for reading only
          gcePersistentDisk:    # the pv is backed by GCE Persitent Disk
            pdName: mongodb
            fsType: ext4
        ---
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: mongodb
          labels:
            app: mongodb
            chapter: "6"
        spec:
          resources:
            requests:
              storage: 1Gi      # requesting 1 GiB of storage
          accessModes:
            - ReadWriteOnce     # a single client for writing and reading
          storageClassName: ""   
       ---
       apiVersion: apps/v1
       kind: ReplicaSet
       ...
       spec:
        ...
         template:
           metadata:
             name: mongodb
             labels:
               app: mongodb
               chapter: "6"
           spec:
             containers:
               - name: mongo
                 image: mongo
                 volumeMounts:
                   - mountPath: /data/db
                     name: mongodb-data
             volumes:
               - name: mongodb-data
                 persistentVolumeClaim:     # refer the pvc by name
                   claimName: mongodb
           
        
        
        
        
        
        
        
        
        
        
        
        
        
        